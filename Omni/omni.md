# Ominous

## Reading List

Papers

#Interaction+Generation:

| Paper                                         | Base Model | Framework | Data | Code | Publication | Preprint                                                                                   | Affiliation |
| --------------------------------------------- | ---------- | --------- | ---- | ---- | ----------- | ------------------------------------------------------------------------------------------ | ----------- |
| Genie 2: A large-scale foundation world model |            |           |      |      |             | [genie2](https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/) | DeepMind    |

#Multimodal #End2end Understanding+Generation:

| Paper                                                                                                                            | Base Model              | Framework                                        | Data                                                      | Code                                                         | Publication | Preprint                                        | Affiliation     |
| -------------------------------------------------------------------------------------------------------------------------------- | ----------------------- | ------------------------------------------------ | --------------------------------------------------------- | ------------------------------------------------------------ | ----------- | ----------------------------------------------- | --------------- |
| UniTok: A Unified Tokenizer for Visual Generation and Understanding                                                              | Llama-2-7B              | TOKEN                                            | mixture (image)                                           | [UniTok](https://github.com/FoundationVision/UniTok)            |             | [2502.20321](https://arxiv.org/abs/2502.20321)     | Bytedance       |
| Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling                                           | DeepSeek-LLM + LLaMAGen | PT + FT (NLL)                                    | mixture (image)                                           | [Janus](https://github.com/deepseek-ai/Janus)                   |             | [2501.17811](https://arxiv.org/abs/2501.17811)     | DeepSeek        |
| MetaMorph: Multimodal Understanding and Generation via Instruction Tuning                                                        | LLaMA-3 + SD            | PT + FT (NLL + cosine sim loss)                  | mixture (image)                                           |                                                              |             | [2412.14164](https://arxiv.org/abs/2412.14164v1)   | Meta            |
| TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation                                                   | LLaVA-1.5 + TokenFlow   | PT + FT                                          | llava/cambrian (image)                                    | [TokenFlow](https://github.com/ByteFlow-AI/TokenFlow)           |             | [2412.03069](https://arxiv.org/abs/2412.03069)     | ByteDance       |
| EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions                                                     | LLaMA3                  | PT + FT                                          | mixture (image, speech)                                   | [emova](https://emova-ollm.github.io/)                          |             | [2409.18042](https://arxiv.org/abs/2409.18042)     | Huawei          |
| MIO: A Foundation Model on Multimodal Tokens                                                                                     | Yi-6B-Base              | PT + FT                                          | mixture (image, speech, audio)                            | [MIO](https://github.com/mio-team/mio)                          |             | [2409.17692](https://arxiv.org/abs/2409.17692)     | HKUST & 01      |
| VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation                                               | LLaMA2 + RQ-VAE         | PT + FT (NLL)                                    | mixture (image)                                           | [vila-u](https://github.com/mit-han-lab/vila-u)                 | ICLR 2025   | [2409.04429](https://arxiv.org/abs/2409.04429)     | MIT             |
| One Single Transformer to Unify Multimodal Understanding and Generation                                                          | Phi + MagViT2           | PT + FT (NLL + MAE-loss)                         | mixture (image)                                           | [Show-o](https://github.com/showlab/Show-o)                     |             | [2408.12528](https://arxiv.org/abs/2408.12528)     | NUS             |
| Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model                                                | - (transfusion)         | PT (NLL-loss + DDPM-loss)                        | self-collect (image)                                      |                                                              |             | [2408.11039](https://www.arxiv.org/abs/2408.11039) | Meta            |
| Anole: An Open, Autoregressive and Native Multimodal Models for Interleaved Image-Text Generation                                | chameleon               | INST (interleaved)                               | mixture (image)                                           | [anole](https://github.com/GAIR-NLP/anole)                      |             | [2407.06135](https://arxiv.org/abs/2407.06135)     | SJTU            |
| Explore the Limits of Omni-modal Pretraining at Scale                                                                            | vicuna                  | PT + INST                                        | mixture (image, video, audio, depth -> text)              | [MiCo](https://github.com/invictus717/MiCo)                     |             | [2406.09412](https://arxiv.org/pdf/2406.09412)     | Shanghai AI Lab |
| Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation                                                        |                         |                                                  |                                                           |                                                              |             |                                                 |                 |
| X-VILA: Cross-Modality Alignment for Large Language Model                                                                        | vicuna + SD             | INST + Diffusion Decoder                         | mixture (image, video, audio)                             |                                                              |             | [2405.19335](https://arxiv.org/abs/2405.19335)     | NVIDIA          |
| Chameleon: Mixed-Modal Early-Fusion Foundation Models                                                                            | - (chameleon)           | PT + FT (AR + image detokenizer)                 | mixture (image)                                           | [chameleon](https://github.com/facebookresearch/chameleon)      |             | [2405.09818](https://arxiv.org/abs/2405.09818)     | Meta            |
| SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation                                            | LLaMA + SD              | PT + INST (interleaved)                          | mixture (image)                                           | [SEED-X](https://github.com/AILab-CVC/SEED-X)                   |             | [2404.14396](https://arxiv.org/abs/2404.14396)     | Tencent         |
| AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling                                                                   | LLaMA2 + SD             | INST + NAR-decoder                               | mixture (image, speech, music)                            | [AnyGPT](https://github.com/OpenMOSS/AnyGPT)                    |             | [2402.12226](https://arxiv.org/abs/2402.12226)     | FDU             |
| World Model on Million-Length Video And Language With Blockwise RingAttention                                                    | LLaMA + VQGAN (LWM)     | PT (long-context)                                | mixture (image, video)                                    | [LWM](https://github.com/LargeWorldModel/LWM)                   |             | [2402.08268](https://arxiv.org/abs/2402.08268)     | UCB             |
| MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer                                  | Vicuna + SD             | PT + INST                                        | mixture (image)                                           | [MM-Interleaved](https://github.com/OpenGVLab/MM-Interleaved)   |             | [2401.10208](https://arxiv.org/abs/2401.10208)     | Shanghai AI Lab |
| Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action                                  | T5X + VQGAN             | PT + INST                                        | mixture (image, audio, video, 3d)                         | [unified-io-2](https://github.com/allenai/unified-io-2)         |             | [2312.17172](https://arxiv.org/abs/2312.17172)     | AI2             |
| VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation                                | LLaMA + SD              | PT + INST (interleaved)                          | mixture (image)                                           | [VL-GPT](https://github.com/AILab-CVC/VL-GPT)                   |             | [2312.09251](https://arxiv.org/abs/2312.09251)     | Tencent         |
| OneLLM: One Framework to Align All Modalities with Language                                                                      | LLaMA2                  | PT + INST (universal encoder + moe projector)    | mixture (image, audio, point, depth, IMU, fMRI -> text)   | [OneLLM](https://github.com/csuhan/OneLLM)                      | CVPR2024    | [2312.03700](https://arxiv.org/abs/2312.03700)     | Shanghai AI lab |
| LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment                            | -                       | INST                                             | mixture (video, infrared, depth, audio -> text)           | [LanguageBind](https://github.com/PKU-YuanGroup/LanguageBind)   | ICLR2024    | [2310.01852](https://arxiv.org/abs/2310.01852)     | PKU             |
| DreamLLM: Synergistic Multimodal Comprehension and Creation                                                                      | Vicuna + SD             | PT + INST with projector (interleaved)           | mixture (image)                                           | [DreamLLM](https://github.com/RunpeiDong/DreamLLM)              | ICLR2024    | [2309.11499](https://arxiv.org/abs/2309.11499)     | MEGVII          |
| NExT-GPT: Any-to-Any Multimodal LLM                                                                                              | Vicuna + SD             | INST with projector                              | mixture (text -> audio/image/video)                       | [NExT-GPT](https://github.com/NExT-GPT/NExT-GPT)                | ICML2024    | [2309.05519](https://arxiv.org/abs/2309.05519)     | NUS             |
| LaVIT: Empower the Large Language Model to Understand and Generate Visual Content,[video version](https://arxiv.org/abs/2402.03161) | LLaMA  + SD            | PT + INST (vector quantization: CE + regression) | mixture (image)                                           | [LaVIT](https://github.com/jy0205/LaVIT)                        | ICLR2024    | [2309.04669](https://arxiv.org/abs/2309.04669)     | Kuaishou        |
| Emu: Generative Pretraining in Multimodality,[v2](https://arxiv.org/abs/2312.13286),[v3](https://github.com/baaivision/Emu3)           | LLaMA + SD              | PT (AR: CE + regression )                       | mixture (image)                                           | [Emu](https://github.com/baaivision/Emu)                        | ICLR2024    | [2307.05222](https://arxiv.org/abs/2307.05222)     | BAAI            |
| Any-to-Any Generation via Composable Diffusion                                                                                   | SD-1.5                  | individual diffusion -> latent attention         | mixture (text -> audio/image/video; image -> audio/video) | [CoDi](https://github.com/microsoft/i-Code/tree/main/i-Code-V3) | NeurIPS2023 | [2305.11846](https://arxiv.org/abs/2305.11846)     | Microsoft       |
| ImageBind: One Embedding Space To Bind Them All                                                                                  | CLIP                    | Contrastive + Diffusion Decoder                  | mixture(image, video, audio, depth)                       | [ImageBind](https://github.com/facebookresearch/ImageBind)      |             | [2305.05665](https://arxiv.org/abs/2305.05665)     | Meta            |

#Streaming #Real-Time #Online

| Paper                                                                                                                                     | Base Model      | Framework                                                    | Data                                                       | Code                                                                                                                     | Publication | Preprint                                                                                                                                                                 | Affiliation     |
| ----------------------------------------------------------------------------------------------------------------------------------------- | --------------- | ------------------------------------------------------------ | ---------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------ | ----------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------- |
| LLaMA-Omni2                                                                                                                               | Qwen2.5         | speech-to-speech                                             | self-construct(InstructS2S-200K)                           | [LLaMA-Omni2](https://github.com/ictnlp/LLaMA-Omni2)                                                                     |             |                                                                                                                                                                          | CAS             |
| Ola: Pushing the Frontiers of Omni-Modal Language Model with Progressive Modality Alignment                                               | Qwen2.5         | VideoLLM + Streaming TTS                                     | mixture                                                    | [Ola](https://github.com/Ola-Omni/Ola)                                                                                      |             | [2502.04328](https://arxiv.org/abs/2502.04328)                                                                                                                              | Tencent         |
| MiniCPM-o 2.6: A GPT-4o Level MLLM for Vision, Speech, and Multimodal Live Streaming on Your Phone                                        | MiniCPM         | chunk-wise for streaming encode and decode + mask            | self-construct                                             | [MiniCPM-o](https://github.com/OpenBMB/MiniCPM-o)                                                                           |             | [2501.notion](https://openbmb.notion.site/MiniCPM-o-2-6-A-GPT-4o-Level-MLLM-for-Vision-Speech-and-Multimodal-Live-Streaming-on-Your-Phone-185ede1b7a558042b5d5e45e6b237da9) | OpenBMB         |
| InternLM-XComposer2.5-OmniLive: A Comprehensive Multimodal System for Long-term Streaming Video and Audio Interactions                    | Qwen2-1.8B      | audio instruction + memory compression                       | self-construct                                             | [InternLM-XComposer-2.5-OmniLive](https://github.com/InternLM/InternLM-XComposer/tree/main/InternLM-XComposer-2.5-OmniLive) |             | [2412.09596](https://arxiv.org/abs/2412.09596)                                                                                                                              | Shanghai AI lab |
| StreamChat: Chatting with Streaming Video                                                                                                 | Qwen2.5         | kv-cache for streaming generation                            | self-construct                                             |                                                                                                                          |             | [2412.08646](https://arxiv.org/abs/2412.08646)                                                                                                                              | CUHK            |
| SALMONN-omni: A Codec-free LLM for Full-duplex Speech Understanding and Generation                                                        | Vicuna          | Streaming Encoder + Streaming TTS                            | mixture                                                    |                                                                                                                          |             | [2411.18138](https://arxiv.org/abs/2411.18138)                                                                                                                              | ByteDance       |
| VideoLLM Knows When to Speak: Enhancing Time-Sensitive Video Comprehension with Video-Text Duet Interaction Format                        | llava-ov        | grounding head                                               | self-construct                                             | [MMDuet](https://github.com/yellow-binary-tree/MMDuet)                                                                      |             | [2411.17991](https://arxiv.org/abs/2411.17991)                                                                                                                              | PKU             |
| Westlake-Omni: Open-Source Chinese Emotional Speech Interaction Large Language Model with Unified Discrete Sequence Modeling              | qwen2           | LLM + AR Decoder                                             | self-construct (Chinese)                                   | [Westlake-Omni](https://github.com/xinchen-ai/Westlake-Omni)                                                                |             |                                                                                                                                                                          | xinchen-ai      |
| Moshi: a speech-text foundation model for real time dialogue                                                                              | Helium-7B       | RQ-Tansformer                                                | self-construct (7m hr (pt) + 2k hr (inst) + 160 hr (tts)) | [moshi](https://github.com/kyutai-labs/moshi/tree/main/moshi)                                                               |             | [2409.pdf](https://kyutai.org/Moshi.pdf)                                                                                                                                    | kyutai          |
| LLaMA-Omni: Seamless Speech Interaction with Large Language Models                                                                        | LLaMA3          | speech-to-speech                                             | self-construct(InstructS2S-200K)                           | [LLaMA-Omni](https://github.com/ictnlp/LLaMA-Omni)                                                                          |             | [2409.06666](https://arxiv.org/abs/2409.06666)                                                                                                                              | CAS             |
| Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming,[v2](https://github.com/gpt-omni/mini-omni2)                           | Qwen2           | audio generation with text instruction + parallel generation | self-construct (VoiceAssistant-400K)                       | [mini-omni](https://github.com/gpt-omni/mini-omni)                                                                          |             | [2408.16725](https://arxiv.org/abs/2408.16725)                                                                                                                              | THU             |
| VideoLLaMB: Long Video Understanding with Recurrent Memory Bridges                                                                        | Vicuna          | scene segment + recurrent                                    | videochat2                                                 | [VideoLLaMB](https://github.com/bigai-nlco/VideoLLaMB)                                                                      |             | [2409.01071](https://arxiv.org/abs/2409.01071)                                                                                                                              | BIGAI           |
| VITA: Towards Open-Source Interactive Omni Multimodal LLM,[v1.5](https://github.com/VITA-MLLM/VITA?tab=readme-ov-file#-whats-new-in-vita-15) | Mixtral-8x7B    | special tokens (<1>: audio; <2>: EOS; <3> text)              | mixture                                                    | [VITA](https://github.com/VITA-MLLM/VITA)                                                                                   |             | [2408.05211](https://arxiv.org/abs/2408.05211)                                                                                                                              | Tencent         |
| VideoLLM-online: Online Large Language Model for Streaming Video                                                                          | Llama2/3        | Multi-turn dialogue + streaming loss                         | Ego4D                                                      | [videollm-online](https://github.com/showlab/videollm-online)                                                               |             | [2406.11816](https://arxiv.org/abs/2406.11816)                                                                                                                              | NUS             |
| RT-DETR: DETRs Beat YOLOs on Real-time Object Detection                                                                                   | Dino + DETR     | anchor-free                                                  | COCO                                                       | [RT-DETR](https://github.com/lyuwenyu/RT-DETR)                                                                              |             | [2304.08069](https://arxiv.org/abs/2304.08069)                                                                                                                              | Baidu           |
| Streaming Dense Video Captioning                                                                                                          | GIT/VidSeq + T5 | cluster visual token (memory)                                |                                                            | [streaming_dvc](https://github.com/google-research/scenic/tree/main/scenic/projects/streaming_dvc)                          | CVPR2024    | [2304.08069](https://arxiv.org/abs/2404.01297)                                                                                                                              | Google          |
| Deformable DETR: Deformable Transformers for End-to-End Object Detection                                                                  | ResNet+DETR     | deformable-attention                                         | COCO                                                       | [Deformable-DETR](https://github.com/fundamentalvision/Deformable-DETR)                                                     | ICLR2021    | [2010.04159](https://arxiv.org/abs/2010.04159)                                                                                                                              | SenseTime       |

#Interactive #Duplex

| Paper                                                                           | Base Model | Framework                  | Data                       | Code                                                | Publication | Preprint                                    | Affiliation |
| ------------------------------------------------------------------------------- | ---------- | -------------------------- | -------------------------- | --------------------------------------------------- | ----------- | ------------------------------------------- | ----------- |
| Enabling Real-Time Conversations with Minimal Training Costs                    | MiniCPM    | AR + special token         | self-curation (Ultra-Chat) |                                                     |             | [2409.11727](https://arxiv.org/abs/2409.11727) | HiT         |
| Beyond the Turn-Based Game: Enabling Real-Time Conversations with Duplex Models | MiniCPM    | AR + time-slice `<idle>` | self-curation (Ultra-Chat) | [duplex-model](https://github.com/thunlp/duplex-model) |             | [2406.15718](https://arxiv.org/abs/2406.15718) | thunlp      |

Projects:

- [2024.09] [Open-Training-Moshi](https://github.com/yangdongchao/Open-Training-Moshi), The reproduce training process for Moshi
- [2024.07] [SAM2](https://github.com/facebookresearch/segment-anything-2), Introducing Meta Segment Anything Model 2 (SAM 2)
  - [2024.08] [segment-anything-2-real-time](https://github.com/Gy920/segment-anything-2-real-time), Run Segment Anything Model 2 on a live video stream
- [2024.06] [LLaVA-Magvit2](https://github.com/lucasjinreal/LLaVA-Magvit2), LLaVA MagVit2: Combines MLLM Understanding and Generation with MagVit2
- [2024.05] [GPT-4o system card](https://openai.com/index/hello-gpt-4o/), We’re announcing GPT-4o, our new flagship model that can reason across audio, vision, and text in real time.

## Dataset

#omininou-modality

- [2024.06] [ShareGPT4Omni Dataset](https://sharegpt4omni.github.io/), ShareGPT4Omni: Towards Building Omni Large Multi-modal Models with Comprehensive Multi-modal Annotations.

#streaming-data

- [2024.06] VideoLLM-online: Online Large Language Model for Streaming Video
- [2024.05] Streaming Long Video Understanding with Large Language Models

## Benchmark

#streaming
- [2025.03] [OmniMMI](https://github.com/OmniMMI/OmniMMI), A Comprehensive Multi-modal Interaction Benchmark in Streaming Video Contexts
- [2025.01] [OVO-Bench](https://github.com/JoeLeelyf/OVO-Bench), OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding?
- [2024.11] [StreamingBench](https://github.com/THUNLP-MT/StreamingBench), StreamingBench evaluates Multimodal Large Language Models (MLLMs) in real-time, streaming video understanding tasks.

#omni
- [2025.03] [ACVUBench](https://github.com/lark-png/ACVUBench), ACVUBench: Audio-Centric Video Understanding Benchmark
- [2024.11] [LongVALE](https://github.com/ttgeng233/LongVALE), LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware Omni-Modal Perception of Long Videos
- [2024.10] [AVHBench](https://github.com/kaist-ami/AVHBench), AVHBench: A Cross-Modal Hallucination Benchmark for Audio-Visual Large Language Models
- [2024.09] [OmniBench](https://github.com/multimodal-art-projection/OmniBench), OmniBench: Towards The Future of Universal Omni-Language Models

#timestampQA
- [2024.06] [VStream-QA](https://invinciblewyq.github.io/vstream-page/), Flash-VStream: Memory-Based Real-Time Understanding for Long Video Streams

#multihop
- [2024.08] [MultiHop-EgoQA](https://qirui-chen.github.io/MultiHop-EgoQA/), Grounded Multi-Hop VideoQA in Long-Form Egocentric Videos

#state#episodic
- [2024.04] [OpenEQA](https://open-eqa.github.io/), OpenEQA: Embodied Question Answering in the Era of Foundation Models
- [2021.10] [Env-QA](https://envqa.github.io/), Env-QA: A Video Question Answering Benchmark for Comprehensive Understanding of Dynamic Environments
